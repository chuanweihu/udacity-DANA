{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 01: Practical Stats\n",
    "\n",
    "###  Lesson 07: Bayes Rule\n",
    "\n",
    "> Learn one of the most popular rules in all of statistics - Bayes rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01. Bayes Rule\n",
    "\n",
    "We're going to talk about perhaps the holy grail of probabilistic inference. It's called **Bayes Rule(贝叶斯定理,Bayes’s Theorem/Bayes’s Law)**.\n",
    "\n",
    "Bayes Rule is based on **Reverend Thomas Bayes**, who used this principle to infer the existence of God, but in doing so, he created a new family of methods that has vastly influenced artificial intelligence and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02. Cancer Test\n",
    "\n",
    "The question being asked is this: \n",
    "\n",
    "1% of the population has cancer. Given that there is a 90% chance that you will test positive if you have cancer and that there is a 90% chance you will test negative if you don't have cancer, what is the probability that you have cancer if you test positive?\n",
    "\n",
    "$ P{(C)}=0.01$\n",
    "\n",
    "The answer is 8%\n",
    "\n",
    "#### 03. Prior And Posterior\n",
    "\n",
    "**Bayes Rule**\n",
    "\n",
    "So this is **the essence of Bayes Rule**, which I'll give to you to you in a second. There's some sort of a prior, of which we mean the probability before you run a test, and then you get some evidence from the test itself.\n",
    "\n",
    "and that all leads you to what's called a **Posterior Probability(后验概率)**. Now this is not really a plus operation. In fact, in reality, it's more like a multiplication, but semantically, what Bayes Rule does is it incorporates some evidence from the test into your **Prior Probability(先验概率)** to arrive at a posterior probability.\n",
    "\n",
    "$ \\text{Prior Probability} \\cdot \\text{Test Evidence} \\to \\text{Posterior Probability}$\n",
    "\n",
    "**Prior**:\n",
    "\n",
    "* $ P(C)=0.01=1\\%$\n",
    "* $ P(\\neg{C})=0.99=99\\%$\n",
    "* $ P(Pos|C)=0.9=90\\%$\n",
    "* $ P(Neg|C)=0.1=10\\%$\n",
    "* $ P(Pos|\\neg{C})=0.1=10\\%$\n",
    "* $ P(Neg|\\neg{C})=0.9=90\\%$\n",
    "\n",
    "**Posterior**\n",
    "\n",
    "* $ P(C|Pos)=P(C)P(Pos|C)=0.01 \\cdot 0.9=0.09$\n",
    "* $ P(\\neg{C}|Pos)=P(\\neg{C})P(Pos|\\neg{C})=0.99 \\cdot 0.1=0.099$\n",
    "\n",
    "#### 04. Normalizing 1\n",
    "\n",
    "The **normalization** proceeds in two steps. We just normalized these guys to keep ratio the same but make sure they add up to 1.\n",
    "\n",
    "**Prior**:\n",
    "\n",
    "* $ P(C)=0.01=1\\%$\n",
    "* $ P(\\neg{C})=0.99=99\\%$\n",
    "* $ P(Pos|C)=0.9=90\\%$\n",
    "* $ P(Neg|C)=0.1=10\\%$\n",
    "* $ P(Pos|\\neg{C})=0.1=10\\%$\n",
    "* $ P(Neg|\\neg{C})=0.9=90\\%$\n",
    "\n",
    "**Joint Probability(联合概率)**\n",
    "\n",
    "* $ P(C,Pos)=P(C)P(Pos|C)=0.01 \\cdot 0.9=0.09$\n",
    "* $ P(\\neg{C},Pos)=P(\\neg{C})P(Pos|\\neg{C})=0.99 \\cdot 0.1=0.099$\n",
    "\n",
    "**Normalizer(规范性)**:\n",
    "\n",
    "$ P(Pos)=P(C, Pos) + P(\\neg{C}, Pos)=0.009+0.099=0.108$\n",
    "\n",
    "#### 05. Normalizing 2\n",
    "\n",
    "**Prior Probability**:\n",
    "\n",
    "* $ P(C)=0.01=1\\%$\n",
    "* $ P(\\neg{C})=0.99=99\\%$\n",
    "* $ P(Pos|C)=0.9=90\\%$\n",
    "* $ P(Neg|C)=0.1=10\\%$\n",
    "* $ P(Pos|\\neg{C})=0.1=10\\%$\n",
    "* $ P(Neg|\\neg{C})=0.9=90\\%$\n",
    "\n",
    "**Joint Probability**\n",
    "\n",
    "* $ P(C,Pos)=P(C) P(Pos|C)=0.01 \\cdot 0.9=0.09$\n",
    "* $ P(\\neg{C},Pos)=P(\\neg{C})P(Pos|\\neg{C})=0.99 \\cdot 0.1=0.099$\n",
    "\n",
    "**Normalizer**\n",
    "\n",
    "* $ P(Pos)=P(C, Pos) + P(\\neg{C}, Pos)=0.009+0.099=0.108$\n",
    "\n",
    "**Posterior Probability**\n",
    "\n",
    "* $ P(C|Pos)=0.009/0.108=0.0833$\n",
    "* $ P(\\neg{C}|Pos)=?$\n",
    "\n",
    "#### 06. Normalizing 3\n",
    "\n",
    "**Posterior Probability**\n",
    "\n",
    "* $ P(C|Pos)=0.009/0.108=0.0833$\n",
    "* $ P(\\neg{C}|Pos)=0.099/0.108=0.9167$\n",
    "\n",
    "\n",
    "#### 07. Total Probability\n",
    "\n",
    "**Posterior Probability**\n",
    "\n",
    "* $ P(C|Pos)=0.009/0.108=0.0833$\n",
    "* $ P(\\neg{C}|Pos)=0.099/0.108=0.9167$\n",
    "* $ P(C|Pos)+P(\\neg{C}|Pos)=1$\n",
    "\n",
    "\n",
    "#### 08. Bayes Rule Diagram\n",
    "\n",
    "**Bayes Rule Diagram**\n",
    "\n",
    "* **Prior**: $ \\boxed{ P(C)}$\n",
    "* **Sensitivity(灵敏度)**: $ \\boxed{P(Pos|C)}$\n",
    "* **Specificity(特异度)**: $ \\boxed{P(Neg|\\neg{C})}$\n",
    "\n",
    "#### 09. Equivalent Diagram\n",
    "\n",
    "**Equivalent Diagram**\n",
    "\n",
    "* **Prior**: $ \\boxed{ P(C)}$\n",
    "* **Sensitivity(灵敏度)**: $ \\boxed{P(Pos|C)}$\n",
    "* **Specificity(特异度)**: $ \\boxed{P(Neg|\\neg{C})}$\n",
    "* $ P(Pos, C) = P(Pos|C) P(C)$\n",
    "* $ P(Neg, C) = P(Neg|C) P(C)$\n",
    "* $ P(Pos, \\neg{C}) = P(Pos|\\neg{C})P(\\neg{C})$\n",
    "* $ P(Neg, \\neg{C}) = P(Neg|\\neg{C})P(\\neg{C})$\n",
    "\n",
    "\n",
    "#### 10. Cancer Probabilities\n",
    "\n",
    "**Given**\n",
    "\n",
    "* **Prior**: $ \\boxed{ P(C)}$\n",
    "* **Sensitivity(灵敏度)**: $ \\boxed{P(Pos|C)}$\n",
    "* **Specificity(特异度)**: $ \\boxed{P(Neg|\\neg{C})}$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "* $ P(\\neg{C})=1-0.01=0.99$\n",
    "* $ P(Neg|C)=1-0.9=0.1$\n",
    "* $ P(Pos|\\neg{C})=1-0.9=0.1$\n",
    "\n",
    "\n",
    "#### 11. Probability Given Test\n",
    "\n",
    "**Given**\n",
    "\n",
    "* $ P(C)=0.01$\n",
    "* $ P(Pos|C)=0.9$\n",
    "* $ P(Neg|\\neg{C})=0.9$\n",
    "\n",
    "* * *\n",
    "\n",
    "* $ P(\\neg{C})=1-0.01=0.99$\n",
    "* $ P(Neg|C)=1-0.9=0.1$\n",
    "* $ P(Pos|\\neg{C})=1-0.9=0.1$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "* $ P(C, Neg)=P(C)P(Neg|C)=0.01 \\cdot 0.1=0.001$\n",
    "* $ P(\\neg{C}, Neg)=P(\\neg{C})P(Neg|\\neg{C})=0.99 \\cdot 0.9=0.891$\n",
    "\n",
    "\n",
    "#### 12. Normalizer\n",
    "\n",
    "**Given**\n",
    "\n",
    "* $ P(C)=0.01$\n",
    "* $ P(Pos|C)=0.9$\n",
    "* $ P(Neg|\\neg{C})=0.9$\n",
    "\n",
    "* * *\n",
    "\n",
    "* $ P(\\neg{C})=1-0.01=0.99$\n",
    "* $ P(Neg|C)=1-0.9=0.1$\n",
    "* $ P(Pos|\\neg{C})=1-0.9=0.1$\n",
    "\n",
    "* * *\n",
    "\n",
    "* $ P(C, Neg)=P(C)P(Neg|C)=0.01 \\cdot 0.1=0.001$\n",
    "* $ P(\\neg{C}, Neg)=P(\\neg{C})P(Neg|\\neg{C})=0.99 \\cdot 0.9=0.891$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "* $ P(Neg)=P(C, Neg)+P(\\neg{C}, Neg)=P(Neg|C)P(C)+P(Neg|\\neg{C})P(\\neg{C})=0.1 \\cdot 0.01 + 0.9 \\cdot 0.99=0.892$\n",
    "\n",
    "\n",
    "#### 13. Normalizing Probability\n",
    "\n",
    "**Given**\n",
    "\n",
    "* $ P(C)=0.01$\n",
    "* $ P(Pos|C)=0.9$\n",
    "* $ P(Neg|\\neg{C})=0.9$\n",
    "\n",
    "* * *\n",
    "\n",
    "* $ P(\\neg{C})=1-0.01=0.99$\n",
    "* $ P(Neg|C)=1-0.9=0.1$\n",
    "* $ P(Pos|\\neg{C})=1-0.9=0.1$\n",
    "\n",
    "* * *\n",
    "\n",
    "* $ P(C, Neg)=P(C)P(Neg|C)=0.01 \\cdot 0.1=0.001$\n",
    "* $ P(\\neg{C}, Neg)=P(\\neg{C})P(Neg|\\neg{C})=0.99 \\cdot 0.9=0.891$\n",
    "\n",
    "* * *\n",
    "\n",
    "* $ P(Neg)=P(C, Neg)+P(\\neg{C}, Neg)=P(Neg|C)P(C)+P(Neg|\\neg{C})P(\\neg{C})=0.1 \\cdot 0.01 + 0.9 \\cdot 0.99=0.892$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "Please specify your answer to at least **4 decimal places**, rounded properly.\n",
    "\n",
    "* $ P(C|Neg)=P(C, Neg)/P(Neg)=0.001/0.892=0.0011$\n",
    "* $ P(\\neg{C}|Neg)=P(\\neg{C}, Neg)/P(Neg)=0.891/0.892=0.9989$\n",
    "\n",
    "Now, what's remarkable about this outcome is really what it means. \n",
    "\n",
    "Before the test, we had a 1% chance of having cancer, now, we have about a 0.9% chance of having cancer. So, a cancer probability **went down** by about a factor of 9. So, the test really helped us gaining confidence that we are cancer-free. \n",
    "\n",
    "Conversely, before we had a 99% chance of being cancer free, now it's 99.89%. So, all the numbers are **working exactly how we expect them to work**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Disease Test 1\n",
    "\n",
    "**Given**\n",
    "\n",
    "$ P(C)=0.1$\n",
    "$ P(Pos|C)=0.9$\n",
    "$ P(Neg|\\neg{C})=0.5$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "$ P(\\neg{C})=1-0.1=0.9$\n",
    "$ P(Neg|C)=1-0.9=0.1$\n",
    "$ P(Pos|\\neg{C})=1-0.5=0.5$\n",
    "\n",
    "* * *\n",
    "\n",
    "$ P(C, Neg)=P(C)P(Neg|C)=?$\n",
    "$ P(\\neg{C}, Neg)=P(\\neg{C})P(Neg|\\neg{C})=?$\n",
    "\n",
    "$ P(Neg)=P(C, Neg)+P(\\neg{C}, Neg)=P(Neg|C)P(C)+P(Neg|\\neg{C})P(\\neg{C})=?$\n",
    "\n",
    "$ P(C|Neg)=P(C, Neg)/P(Neg)=?$\n",
    "$ P(\\neg{C}|Neg)=P(\\neg{C}, Neg)/P(Neg)=?$\n",
    "\n",
    "#### 15. Disease Test 2\n",
    "\n",
    "--**snip**--\n",
    "\n",
    "#### 16. Disease Test 3\n",
    "\n",
    "--**snip**--\n",
    "\n",
    "#### 17. Disease Test 4\n",
    "\n",
    "--**snip**--\n",
    "\n",
    "#### 18. Disease Test 5\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "* $ P(C, Neg)=P(C)P(Neg|C)=0.1 \\cdot 0.1=0.01$\n",
    "* $ P(\\neg{C}, Neg)=P(\\neg{C})P(Neg|\\neg{C})=0.9 \\cdot 0.5=0.45$\n",
    "\n",
    "* * *\n",
    "\n",
    "$ P(Neg)=P(C, Neg)+P(\\neg{C}, Neg)=P(Neg|C)P(C)+P(Neg|\\neg{C})P(\\neg{C})=0.1 \\cdot 0.1 + 0.9 \\cdot 0.5=0.46$\n",
    "\n",
    "* * *\n",
    "\n",
    "* $ P(C|Neg)=P(C, Neg)/P(Neg)=0.01/0.46=0.0217$\n",
    "* $ P(\\neg{C}|Neg)=P(\\neg{C}, Neg)/P(Neg)=0.45/0.46=0.9783$\n",
    "\n",
    "\n",
    "#### 19. Disease Test 6\n",
    "\n",
    "**Given**\n",
    "\n",
    "* $ P(C)=0.1$\n",
    "* $ P(Pos|C)=0.9$\n",
    "* $ P(Neg|\\neg{C})=0.5$\n",
    "\n",
    "* * *\n",
    "\n",
    "* $ P(\\neg{C})=1-0.1=0.9$\n",
    "* $ P(Neg|C)=1-0.9=0.1$\n",
    "* $ P(Pos|\\neg{C})=1-0.5=0.5$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "* $ P(C, Pos)=P(C)P(Pos|C)=0.1 \\cdot 0.9=0.09$\n",
    "* $ P(\\neg{C}, Pos)=P(\\neg{C})P(Pos|\\neg{C})=0.9 \\cdot 0.5=0.45$\n",
    "* $ P(Pos)=P(C, Pos)+P(\\neg{C}, Pos)=P(Pos|C)P(C)+P(Pos|\\neg{C})P(\\neg{C})=0.54$\n",
    "* $ P(C|Pos)=P(C, Pos)/P(Pos)=0.09/0.54=0.1667$\n",
    "* $ P(\\neg{C}|Pos)=P(\\neg{C}, Pos)/P(Pos)=0.45/0.54=0.833$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20. Bayes Rule Summary\n",
    "\n",
    "In **Bayes rule**, we have a **hidden variable** we care about--whether they have cancer or not. But **we can't measure it directly and instead we have a test**.\n",
    "\n",
    "We have a prior of how frequent this variable is true and the test is generally characterized by how often it says positive when the variable is true and how often it is negative and the variable is false.\n",
    "\n",
    "**Bayes' rule** looks like this:\n",
    "\n",
    "$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$\n",
    "\n",
    "Bayes rule takes a prior, **multiplies** in the measurement, which in this case we assume to be the positive measurement to give us a new variable and does the same for all actual measurement, given the opposite assumption about our hidden variable of cancer and that multiplication gives us this guy over here.\n",
    "\n",
    "We **add** those two things up and then it gives us a new variable and then we **divide** these guys to arrive the **best estimate of the hidden variable c** given our test result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21. Robot Sensing 1\n",
    "\n",
    "In this case, you are a robot. This robot lives in a world of exactly two places. There is a red place and a green place, R and G.\n",
    "\n",
    "Now, I say initially, this robot has no clue where it is, so the prior probability for either place, red or green, is 0.5. It also has a sensor as it can see through its eyes, but his sensor seems to be somewhat unreliable. So, the probability of seeing red at the red grid cell is 0.8, and the probability of seeing green at the green cell is also 0.8. \n",
    "\n",
    "Now, I suppose the robot sees red. What are now the posterior probabilities that the robot is at the red cell given that it just saw red and conversely what's the probability that it's at the green cell even though it saw red.\n",
    "\n",
    "**Given**\n",
    "\n",
    "$ P(\\text{at R})=P(\\text{at G})=0.5$\n",
    "$ P(\\text{See R}|\\text{at R})=0.8$\n",
    "$ P(\\text{see G}|\\text{at G})=0.8$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "$ P(\\text{at R}|\\text{See R})=P(\\text{See R}|\\text{at R})P(\\text{at R})/P(\\text{See R})=?$\n",
    "$ P(\\text{at G}|\\text{See R})=P(\\text{See R}|\\text{at G})P(\\text{at G})/P(\\text{See R})=?$\n",
    "\n",
    "**Solution**\n",
    "\n",
    "The step-by-step breakdown of the solution is pretty quick. Let's recap what's covered in the solution video.\n",
    "\n",
    "Let's start with what we know:\n",
    "\n",
    "1. Prior Probabilities\n",
    "The robot is perfectly ignorant about where it is, so prior probabilities are as follows:\n",
    "\n",
    "$ P(\\text{at R})=P(\\text{at G})=0.5$\n",
    "\n",
    "2. Conditional Probabilities\n",
    "\n",
    "The robot's sensors are not perfect. Just because the robot sees red does not mean the robot is at Red.\n",
    "\n",
    "$ P(\\text{See R}|\\text{at R})=0.8$\n",
    "$ P(\\text{see G}|\\text{at G})=0.8$\n",
    "\n",
    "3. Posterior Probabilities\n",
    "From these prior and posterior probabilities we are asked to calculate the following posterior probabilities after the robot sees red:\n",
    "\n",
    "$ P(\\text{at R}|\\text{See R})=P(\\text{See R}|\\text{at R})P(\\text{at R})/P(\\text{See R})=\\frac{0.8 \\cdot 0.5}{P(\\text{See R})}=0.8$\n",
    "$ P(\\text{at G}|\\text{See R})=P(\\text{See R}|\\text{at G})P(\\text{at G})/P(\\text{See R})=\\frac{0.2 \\cdot 0.5}{P(\\text{See R})}=0.2$\n",
    "\n",
    "**Why is P(See R) 0.5?**\n",
    "\n",
    "* Argument 1: Intuitive\n",
    "\n",
    "Of course it's 0.5! What else could it be? The robot had a 50% belief that it was in red and a 50% belief that it was in green. Sure, its sensors are unreliable but that unreliability is symmetric and not biased towards mistakenly seeing either color.\n",
    "\n",
    "So whatever the probability of seeing red is, that will also be the probability of seeing green. Since these two colors are the only possible colors the probability MUST be 50% for each!\n",
    "\n",
    "* Argument 2: Mathematical (Law of Total Probability)\n",
    "There are exactly two situations where the robot would see Red.\n",
    "    1. When the robot is in a red square and its sensors work correctly.\n",
    "    2. When the robot is in a green square and its sensors make a mistake.\n",
    "    3. I just need to add up these two probabilities to get the total probability of seeing red.\n",
    "$ P(\\text{See R}, \\text{at R})=P(\\text{See R}|\\text{at R})P(\\text{at R})=0.8 \\cdot 0.5=0.4$\n",
    "$ P(\\text{See R}, \\text{at G})=P(\\text{See R}|\\text{at G})P(\\text{at G})=0.2 \\cdot 0.5=0.1$\n",
    "$ P(\\text{See R}) = P(\\text{See R}|\\text{at R})P(\\text{at R}) + P(\\text{See R}|\\text{at G})P(\\text{at G})=0.8 \\cdot 0.5 + 0.2 \\cdot 0.5)=0.5$\n",
    "\n",
    "\n",
    "#### 22. Robot Sensing 2\n",
    "\n",
    "**Given**\n",
    "\n",
    "$ P(\\text{at R})=0$\n",
    "$ P(\\text{at G})=1$\n",
    "$ P(\\text{See R}|\\text{at R})=0.8$\n",
    "$ P(\\text{see G}|\\text{at G})=0.8$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "$ P(\\text{at R}|\\text{See R})=\\frac{P(\\text{See R}|\\text{at R})P(\\text{at R})}{P(\\text{See R})}=?$\n",
    "$ P(\\text{at G}|\\text{See R})=\\frac{P(\\text{See R}|\\text{at G})P(\\text{at G})}{P(\\text{See R})}=?$\n",
    "\n",
    "**Solution**\n",
    "\n",
    "$ P(\\text{See R}, \\text{at R})=P(\\text{See R}|\\text{at R})P(\\text{at R})=0.8 \\cdot 0=0$\n",
    "$ P(\\text{See R}, \\text{at G})=P(\\text{See R}|\\text{at G})P(\\text{at G})=0.2 \\cdot 1=0.2$\n",
    "$ P(\\text{See R}) = P(\\text{See R}|\\text{at R})P(\\text{at R}) + P(\\text{See R}|\\text{at G})P(\\text{at G})=0.8 \\cdot 0 + 0.2 \\cdot 1)=0.2$\n",
    "\n",
    "$ P(\\text{at R}|\\text{See R})=\\frac{P(\\text{See R}|\\text{at R})P(\\text{at R})}{P(\\text{See R})}=0$\n",
    "$ P(\\text{at G}|\\text{See R})=\\frac{P(\\text{See R}|\\text{at G})P(\\text{at G})}{P(\\text{See R})}=1$\n",
    "\n",
    "\n",
    "#### 23. Robot Sensing 3\n",
    "\n",
    "**Given**\n",
    "\n",
    "$ P(\\text{at R})=0.5$\n",
    "$ P(\\text{at G})=0.5$\n",
    "$ P(\\text{See R}|\\text{at R})=0.8$\n",
    "$ P(\\text{see G}|\\text{at G})=0.5$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "$ P(\\text{at R}|\\text{See R})=\\frac{P(\\text{See R}|\\text{at R})P(\\text{at R})}{P(\\text{See R})}=?$\n",
    "$ P(\\text{at G}|\\text{See R})=\\frac{P(\\text{See R}|\\text{at G})P(\\text{at G})}{P(\\text{See R})}=?$\n",
    "\n",
    "**Solution**\n",
    "\n",
    "$ P(\\text{See R}, \\text{at R})=P(\\text{See R}|\\text{at R})P(\\text{at R})=0.8 \\cdot 0.5=0.4$\n",
    "$ P(\\text{See R}, \\text{at G})=P(\\text{See R}|\\text{at G})P(\\text{at G})=0.5 \\cdot 0.5=0.25$\n",
    "$ P(\\text{See R}) = P(\\text{See R}|\\text{at R})P(\\text{at R}) + P(\\text{See R}|\\text{at G})P(\\text{at G})=0.8 \\cdot 0.5 + 0.5 \\cdot 0.5)=0.65$\n",
    "\n",
    "$ P(\\text{at R}|\\text{See R})=\\frac{P(\\text{See R}|\\text{at R})P(\\text{at R})}{P(\\text{See R})}=0.4/0.65=0.6153$\n",
    "$ P(\\text{at G}|\\text{See R})=\\frac{P(\\text{See R}|\\text{at G})P(\\text{at G})}{P(\\text{See R})}=0.25/0.65=0.3847$\n",
    "\n",
    "\n",
    "#### 24. Robot Sensing 4\n",
    "\n",
    "Suppose there are 3 places in the world, not just 2. There are a red one and 2 green ones. And for simplicity, we'll call them A, B, and C.\n",
    "\n",
    "Let's assume that all of them have the same prior probability of 1/3 or 0.333, so on. Let's say the robot sees red, and as before, the probability of seeing red in Cell A is 0.9. The probability of seeing green in Cell B 0.9. Probability of seeing green in Cell C is also 0.9.\n",
    "\n",
    "So let me ask you, what is the joint of being in Cell A after having seen the red color?\n",
    "\n",
    "**Given**\n",
    "\n",
    "$ P(A)=P(B)=P(C)=1/3$\n",
    "$ P(\\text{See R}|A)=0.9$\n",
    "$ P(\\text{see G}|B)=0.9$\n",
    "$ P(\\text{see G}|C)=0.9$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "$ P(A, \\text{See R})=P(A)P(\\text{See R}|A)=0.3$\n",
    "\n",
    "\n",
    "#### 25. Robot Sensing 5\n",
    "\n",
    "**Given**\n",
    "\n",
    "$ P(A)=P(B)=P(C)=1/3$\n",
    "$ P(\\text{See R}|A)=0.9$\n",
    "$ P(\\text{see G}|B)=0.9$\n",
    "$ P(\\text{see G}|C)=0.9$\n",
    "\n",
    "$ P(A, \\text{See R})=P(A)P(\\text{See R}|A)=0.3$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "$ P(B, \\text{See R})=P(B)P(\\text{See R}|B)=0.033$\n",
    "\n",
    "\n",
    "#### 26. Robot Sensing 6\n",
    "\n",
    "**Given**\n",
    "\n",
    "$ P(A)=P(B)=P(C)=1/3$\n",
    "$ P(\\text{See R}|A)=0.9$\n",
    "$ P(\\text{see G}|B)=0.9$\n",
    "$ P(\\text{see G}|C)=0.9$\n",
    "\n",
    "$ P(A, \\text{See R})=P(A)P(\\text{See R}|A)=0.3$\n",
    "$ P(B, \\text{See R})=P(B)P(\\text{See R}|B)=0.033$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "$ P(C, \\text{See R})=P(C)P(\\text{See R}|C)=0.033$\n",
    "\n",
    "\n",
    "#### 27. Robot Sensing 7\n",
    "\n",
    "**Given**\n",
    "\n",
    "$ P(A)=P(B)=P(C)=1/3$\n",
    "$ P(\\text{See R}|A)=0.9$\n",
    "$ P(\\text{see G}|B)=0.9$\n",
    "$ P(\\text{see G}|C)=0.9$\n",
    "\n",
    "$ P(A, \\text{See R})=P(A)P(\\text{See R}|A)=0.3$\n",
    "$ P(B, \\text{See R})=P(B)P(\\text{See R}|B)=0.033$\n",
    "$ P(C, \\text{See R})=P(C)P(\\text{See R}|C)=0.033$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "$ P(\\text{See R})=P(A, \\text{See R})+P(B, \\text{See R})+P(C, \\text{See R})=0.366$\n",
    "\n",
    "\n",
    "#### 28. Robot Sensing 8\n",
    "\n",
    "**Given**\n",
    "\n",
    "$ P(A)=P(B)=P(C)=1/3$\n",
    "$ P(\\text{See R}|A)=0.9$\n",
    "$ P(\\text{see G}|B)=0.9$\n",
    "$ P(\\text{see G}|C)=0.9$\n",
    "\n",
    "$ P(A, \\text{See R})=P(A)P(\\text{See R}|A)=0.3$\n",
    "$ P(B, \\text{See R})=P(B)P(\\text{See R}|B)=0.033$\n",
    "$ P(C, \\text{See R})=P(C)P(\\text{See R}|C)=0.033$\n",
    "$ P(\\text{See R})=P(A, \\text{See R})+P(B, \\text{See R})+P(C, \\text{See R})=0.366$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "$ P(A|\\text{See R})=\\frac{P(A, \\text{See R})}{P(R)}=0.3/0.366=0.818$\n",
    "$ P(B|\\text{See R})=\\frac{P(B, \\text{See R})}{P(R)}=0.033/0.366=0.091$\n",
    "$ P(C|\\text{See R})=\\frac{P(C, \\text{See R})}{P(R)}=0.033/0.366=0.091$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 29. Generalizing\n",
    "\n",
    "**Generalizing(泛化)**\n",
    "\n",
    "In **Bayes Rule**, there will be more than just 2 underlying causes of cancer/non cancer. There might be 3, 4, or 5, any number. We can **apply** exactly the same math, but we have to keep track of more values. And this means that our **measurement probability** will be more elaborate. I have to give you more information, but the math remains exactly the same. \n",
    "\n",
    "We can now deal with very large problems that have many possible hidden causes of where the world might be, and we can still **apply Bayes Rule** to find all of these numbers.\n",
    "\n",
    "\n",
    "#### 30. Sebastian At Home\n",
    "\n",
    "So let's say, I'm gone `60%` of my time and I'm at home only `40%` of my time. Now at summer, I live in California and it truly doesn't rain in the summer. Whereas in many of the countries I have traveled to, there's a much higher chance of rain. \n",
    "\n",
    "So let's now say, I lie in my bed, here I am lying in bed, and I wake up and I open the window and I see it's raining. \n",
    "\n",
    "Let's now apply Bayes rule--What do you think is the probability I'm home now that I see it's raining--just give me this one number.\n",
    "\n",
    "**Given**\n",
    "\n",
    "* $ P(gone)=0.6$\n",
    "* $ P(home)=0.4$\n",
    "* $ P(rain|home)=0.01$\n",
    "* $ P(rain|gone)=0.3$\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "$ P(home|rain)=0.4 \\cdot 0.01/(0.4 \\cdot 0.01+0.6 \\cdot 0.3)=0.0217$\n",
    "\n",
    "If so, you now understand something that's really interesting. You're able to look at a **hidden variable**, understand how a **test** can give you information back about this hidden variable and that's really cool because it allows you to apply the same scheme to great many practical problems in the world.\n",
    "\n",
    "\n",
    "#### 31. Learning Objectives - Conditional Probability\n",
    "\n",
    "Learning Objectives - Conditional Probability\n",
    "We use the notation where\n",
    "\n",
    "* $ P(A)$ means \"the probability of A\"\n",
    "* $ P(\\neg A)$ means \"the probability of NOT A\"\n",
    "* $ P(A,B)$ means \"the probability of A **and** B\" and\n",
    "* $ P(A∣B)$ means \"the probability of A **given** B.\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "1. If A and B are independent events and P(A) = 0.2 and P(B) = 0.1, what is P(A,B)?\n",
    "    * $ P(A,B)=0.2 \\cdot 0.1 = 0.02$\n",
    "\n",
    "2. If A and B are NOT independent events, and P(A) = 0.2 and P(B) = 0.1, what is P(A, B)?\n",
    "    * $ P(A,B)=?(\\text{Not enough information to answer})$\n",
    "\n",
    "3. If A and B are NOT independent events, and P(A) = 0.2, P(B) = 0.1, and $ P(B|A) = 0.3$ what is P(A|B)?\n",
    "    * $ P(A|B)=P(A)\\frac{P(B|A)}{P(B)}=0.2 \\cdot 0.3 / 0.1=0.6$\n",
    "\n",
    "**Note**:\n",
    "\n",
    "* The remaining questions deal with two coins.\n",
    "    * Coin 1 is fair. When flipped it has a probability of 0.5 for heads and 0.5 for tails.\n",
    "    * Coin 2 is biased. When flipped it has a probability of 0.9 for heads and 0.1 for tails.\n",
    "\n",
    "4. You grab one of these two coins at Random (equally likely that you grabbed coin 1 or 2) and you flip it. What's the probability it comes up heads?\n",
    "$ P(H)=0.2+0.45=0.7$\n",
    "\n",
    "**True Table**\n",
    "\n",
    "| **PICK** | **FLIP** | **P** |\n",
    "| --- | --- | --- |\n",
    "| 1 | H | 0.25|\n",
    "| 1 | T | 0.25 |\n",
    "| 2 | H | 0.45 |\n",
    "| 2 | T | 0.05 |\n",
    "\n",
    "5. You grab a coin at Random and flip it twice. What's the probability that it comes up tails both times?\n",
    "$ P(T,T)=0.0125+0.005=0.13$\n",
    "\n",
    "**True Table**\n",
    "\n",
    "| **PICK** | **FLIP-Once** | **FLIP-Twice** | **P** |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | H | H |  |\n",
    "| 1 | H | T |  |\n",
    "| 1 | T | H |  |\n",
    "| 1 | T | T | $ 0.5 \\cdot 0.5 \\cdot 0.5=0.0125$ |\n",
    "| 2 | H | H |  |\n",
    "| 2 | H | T |  |\n",
    "| 2 | T | H |  |\n",
    "| 2 | T | T | $ 0.5 \\cdot 0.1 \\cdot 0.1=0.005$ |\n",
    "\n",
    "\n",
    "#### 32. Reducing Uncertainty\n",
    "\n",
    "Let's talk a bit more about why **uncertainty** is so important in the field of robotics and self-driving cars.\n",
    "\n",
    "We know that **measurements** like the speed, the direction, and the location of a car are challenging to measure and we can't measure them perfectly. There's some uncertainty in each of these measurements. We also know that many of these measurements affect one another.\n",
    "\n",
    "For example, if we are uncertain about the location of a car, we can reduce that uncertainty by collecting data about the car's surroundings and its movement. Self-driving cars measure all of these things from a car's speed, to the scenery and objects that surround it, with sensors. And though these sensor measurements aren't perfect, when the information they provide is combined **using conditional probability and something called Bayes rule**, they can form a reliable representation of a car's position, its movement and its environment.\n",
    "\n",
    "#### 33. Bayes' Rule and Robotics\n",
    "\n",
    "**Bayes rule** is extremely important in robotics and it can be described in one sentence. \n",
    "\n",
    "Given an **initial prediction**, if we gather additional related data, data that our initial prediction depends on, we can **improve that prediction**.\n",
    "\n",
    "For example, let's say our initial prediction also known as a **prior belief**, is an estimate of a car's location on a road. This might be the location given by a slightly inaccurate GPS signal. Then, we use sensors to gather data about the car's surroundings and how the car is moving.\n",
    "\n",
    "How do you think this sensor data can help us improve our initial location prediction?\n",
    "\n",
    "\n",
    "#### 34. Learning from Sensor Data\n",
    "\n",
    "**Initial Scenario**\n",
    "\n",
    "We know a little bit about the map of the road that our car is on (pictured above). We also have an **initial GPS measurement**; the GPS signal says the car is at the **red dot**. However, this GPS measurement is inaccurate up to about 5 meters. So, the vehicle could be located anywhere within a 5m radius circle around the dot.\n",
    "\n",
    "**Sensors**\n",
    "\n",
    "Then we gather data from the car's sensors. Self-driving cars mainly use three types of sensors to observe the world:\n",
    "\n",
    "* **Camera**, which records video,\n",
    "* **Lidar**, which is a light-based sensor, and\n",
    "* **Radar**, which uses radio waves.\n",
    "\n",
    "All of these sensors detect surrounding objects and scenery.\n",
    "\n",
    "Autonomous cars also have lots of internal sensors that measure things like the **speed and direction** of the car's movement, the **orientation** of its wheels, and even the **internal temperature** of the car!\n",
    "\n",
    "\n",
    "#### 35. Using Sensor Data\n",
    "\n",
    "Once we gather sensor data about the car's surroundings and its movement, we can then use this information to improve our initial location prediction.\n",
    "\n",
    "For example, say we sense lane markers and specific terrain, and we say, Actually, we know from previously collected data that if we sense landlines close to the sides of the car, the car is probably located in the center of the lane. We also know that if we sense that our tires are pointing to the right, we're probably on a curved section of the road.\n",
    "\n",
    "So this sensor data, combined with what we already know about the road and the car, gives us more information about where our location is most likely to be. \n",
    "\n",
    "So using the sensor information, we can improve our initial prediction and better estimate our car's location. Bayes rule gives us a mathematical way to correct our measurements, and let's us move from an uncertain prior belief to something more and more probable.\n",
    "\n",
    "\n",
    "#### 36. Learning Objectives - Bayes' Rule\n",
    "\n",
    "The following questions will help you review what you learned in the Bayes' Rule lesson.\n",
    "\n",
    "* Prior knowledge\n",
    "\n",
    "For questions 1-3, assume you already have the following knowledge:\n",
    "You’re interested in finding out the probability of a car stopping if it sees a yellow traffic light.\n",
    "\n",
    "* Past data tells you that the probability of a car stopping at a traffic light intersection is $ P(S)=0.40$.\n",
    "* You also know that the past probability of a traffic light being yellow (as opposed to red or green) is $ P(Y)=0.10$.\n",
    "\n",
    "**Traffic Light q1**\n",
    "\n",
    "* When a car is stopped at an intersection, data shows that 12% of the time the light is yellow. So if we know a car is stopped, there's a 12% chance the light is yellow. This is called a conditional probability. Given P(S) and P(Y) above, how would you represent this conditional probability in notation?\n",
    "ANSWER: $ P(Y|S)=0.12$\n",
    "\n",
    "**Traffic Light q2**\n",
    "\n",
    "* Using what you know from question 1, answer the following: if the traffic light is yellow, what is the chance that the car will stop?\n",
    "ANSWER: $ P(S|Y)=\\frac{P(S)P(Y|S)}{P(Y)}=0.4 \\cdot 0.12 / 0.1 = 0.48$\n",
    "\n",
    "**Traffic Light q3**\n",
    "\n",
    "* Knowing that a car stopping at an intersection and the presence of a yellow traffic light are related events, what are P(S) and P(Y) known as?\n",
    "ANSWER: Prior probabilities\n",
    "\n",
    "* * *\n",
    "\n",
    "**Questions 4 and 5 are different scenarios**\n",
    "\n",
    "* Prior knowledge for question 4:\n",
    "\n",
    "On a four-lane highway, cars are either going fast or not fast. Faster cars should go in the leftmost lanes.\n",
    "* At any given time, 20% of cars are in the left-most lane.\n",
    "$ P(L)=0.2$\n",
    "* Overall, 40% of cars on the highway are classified as going fast.\n",
    "$ P(F)=0.4$\n",
    "* Out of all the cars in the leftmost lane, 90% are going fast.\n",
    "$ P(F|L)=0.9$\n",
    "\n",
    "**Bayes q4**\n",
    "\n",
    "Given the above information, if a car is going fast, what is the probability that it will be in the leftmost lane?\n",
    "\n",
    "$ P(L|F)=P(L)P(F|L)/P(F)=0.2 \\cdot 0.9 / 0.4=0.45$\n",
    "\n",
    "* * *\n",
    "\n",
    "Bayes' rule is not only used to incorporate sensor data into an estimate; it’s also often used to incorporate test data into a medical diagnosis.\n",
    "\n",
    "Prior knowledge for question 5:\n",
    "\n",
    "* 1% of all people have cancer.\n",
    "$ P(C)=0.01$\n",
    "* 90% of people who have cancer test positive when given a cancer-detecting blood test, meaning the test detects cancer 90% of the time.\n",
    "$ P(P)=0.9$\n",
    "* 5% of people will have false positives, meaning that 5% of the time, this test will produce a positive result when people do not have cancer.\n",
    "$ P(P|\\neg{C})=0.05$\n",
    "\n",
    "**Bayes q5**\n",
    "\n",
    "Given the above data, what is the probability that a person has cancer if they have a positive cancer-test result? (Note: answers are rounded to the nearest 4th decimal place).\n",
    "\n",
    "* $ P(C)=0.01$\n",
    "* $ P(Pos|C)=0.9$\n",
    "* $ P(Pos|\\neg{C})=0.05$\n",
    "* $ P(\\neg{C})=1-0.01=0.99$\n",
    "* $ P(Neg|C)=1-0.9=0.1$\n",
    "* $ P(Neg|\\neg{C})=1-0.05=0.95$\n",
    "* $ P(C, Pos)=P(C)P(Pos|C)=0.01 \\cdot 0.9=0.009$\n",
    "* $ P(\\neg{C}, Pos)=P(\\neg{C})P(Pos|\\neg{C})=0.99 \\cdot 0.05=0.0495$\n",
    "* $ P(Pos)=P(C, Pos)+P(\\neg{C}, Pos)=P(Pos|C)P(C)+P(Pos|\\neg{C})P(\\neg{C})=0.0585$\n",
    "* $ P(C|Pos)=P(C, Pos)/P(Pos)=0.009/0.0585=0.1538$\n",
    "\n",
    "\n",
    "#### 37. Bayes Rule Conclusion\n",
    "\n",
    "At this point, you've now gained a ton of valuable knowledge about probability, conditional probability and Bayes rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 38. Glossary\n",
    "\n",
    "* Joint Probability(联合概率)\n",
    "* Normalizer(规范性)\n",
    "* Prior Probability(先验概率)\n",
    "* Posterior Probability(后验概率)\n",
    "* Bayes Rule(贝叶斯定理,Bayes’s Theorem/Bayes’s Law)\n",
    "* Sensitivity(灵敏度)\n",
    "* Specificity(特异度)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Web Crawl\n",
    "\n",
    "> In this lesson we will apply the skills learned previously by writing a web crawler that explores Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Introduction\n",
    "\n",
    "**Automating the Wikipedia Crawl**\n",
    "\n",
    "Go to a Wikipedia page you find interesting, or just a random one and click the first link. Then on that page click the first link in the main body of the article text and just keep going. You could try it multiple times for different pages and see whether you get different behaviour. We're going to work on automating this process, ending up with a program that will go through Wikipedia for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking the Problem into Steps\n",
    "\n",
    "### The Manual Process\n",
    "\n",
    "1. Open an article\n",
    "2. Find the first link in the article\n",
    "3. Follow the link\n",
    "4. Record the link in the article_chain data structure.\n",
    "5. Repeat this process until we reach the Philosophy article, or get stuck in an article cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laying the Groundwork\n",
    "\n",
    "* **The Sequence of Steps**\n",
    "    1. Find target tags\n",
    "    2. Get html\n",
    "    3. Parse html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find target tag\n",
    "start_url = 'https://en.wikipedia.org/wiki/Film'\n",
    "target_url = \"https://en.wikipedia.org/wiki/Philosophy\"\n",
    "\n",
    "first_xpath = '/html/body/div[3]/div[3]/div[4]/div/p[2]/a[1]'\n",
    "first_element = '<a href=\"/wiki/Visual_art\" class=\"mw-redirect\" title=\"Visual art\">visual art</a>'\n",
    "first_link = 'https://en.wikipedia.org/wiki/Visual_art'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HTML\n",
    "# Tool：requests(文本)\n",
    "import requests\n",
    "proxies = {\"http\": \"http://127.0.0.1:1087\",\n",
    "           \"https\": \"http://127.0.0.1:1087\",\n",
    "          }\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "headers = {'User-Agent':user_agent}\n",
    "response = requests.get(start_url, proxies=proxies, headers=headers)\n",
    "html = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Limited_animation'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse HTML\n",
    "# Tool：BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html5lib\")\n",
    "\n",
    "content_div = soup.find(id=\"mw-content-text\").find(class_=\"mw-parser-output\")\n",
    "for element in content_div.find_all(\"p\", recursive=False):\n",
    "    if element.find(\"a\", recursive=False):\n",
    "        first_relative_link = element.find('a', recursive=False).get('href')\n",
    "\n",
    "\n",
    "# # as we get links in Wikipedia articles are relative urls rather than absolute urls\n",
    "import urllib\n",
    "urllib.parse.urljoin('https://en.wikipedia.org/', first_relative_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the Program\n",
    "\n",
    "**Design**\n",
    "\n",
    "* Looping\n",
    "* Data structures\n",
    "* Steps to perform\n",
    "* Specific extra(slowing dow)\n",
    "\n",
    "**loop**\n",
    "\n",
    "Steps of loop:\n",
    "1. Find the first link in the current article's HTML\n",
    "2. Download the HTML for the current article\n",
    "3. Add the first link from the current article to article_chain\n",
    "4. Pause for a couple seconds so we don't flood Wikipedia with requests.\n",
    "\n",
    "The program should end the while loop when:\n",
    "1. we reach Philosophy,\n",
    "2. we reach a page we've already visited, hence find ourselves in a cycle of articles (like the case of Chair,\n",
    "3. we go on for too long (we think that 25 steps is plenty, but you can adjust this if you like), or \n",
    "4. we find a page that has no links on it - we simply can't keep going in this case.\n",
    "\n",
    "**Pseudo Code**\n",
    "\n",
    "```\n",
    "page = a random starting page\n",
    "article_chain = []\n",
    "while title of page isn't 'Philosophy' and we have not discovered a cycle:\n",
    "    append page to article_chain\n",
    "    download the page content\n",
    "    find the first link in the content\n",
    "    page = that link\n",
    "    pause for a second\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Program\n",
    "\n",
    "**Write code** \n",
    "\n",
    "* Code to control loop\n",
    "* Steps inside loop\n",
    "* Planned find_first_link\n",
    "* Wrote find_first_link\n",
    "* Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework of program\n",
    "import time\n",
    "def web_crawl(start_url, target_url):\n",
    "    article_chain = [start_url]\n",
    "    while continue_crawl(article_chain, target_url):\n",
    "        print(article_chain[-1])\n",
    "        first_link = find_first_link(article_chain[-1])\n",
    "        if first_link:\n",
    "            article_chain.append(first_link)\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            print(\"We've arrived at an article with no links, aborting search!\")\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branch of program\n",
    "import urllib\n",
    "def continue_crawl(search_history, target_url, max_steps=25):\n",
    "    if search_history[-1] == target_url:\n",
    "        print(\"We've found the target article!\")\n",
    "        return False\n",
    "    elif len(search_history) > max_steps:\n",
    "        print(\"The search has gone on suspiciously long, aborting search!\")\n",
    "        return False\n",
    "    elif search_history[-1] in search_history[:-1]:\n",
    "        print(\"We've arrived at an article we've already seen, aborting search!\")\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def find_first_link(url):\n",
    "    response = requests.get(url, proxies=proxies, headers=headers)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, \"html5lib\")\n",
    "    \n",
    "    content_div = soup.find(id=\"mw-content-text\").find(class_=\"mw-parser-output\")\n",
    "    \n",
    "    article_link = None\n",
    "\n",
    "    for element in content_div.find_all(\"p\", recursive=False):\n",
    "        if element.find(\"a\", recursive=False):\n",
    "            article_link = element.find(\"a\", recursive=False).get('href')\n",
    "            break\n",
    "\n",
    "    if not article_link:\n",
    "        return\n",
    "    first_link = urllib.parse.urljoin('https://en.wikipedia.org/', article_link)\n",
    "\n",
    "    return first_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Film\n",
      "https://en.wikipedia.org/wiki/Visual_art\n",
      "https://en.wikipedia.org/wiki/Art#Forms,_genres,_media,_and_styles\n",
      "https://en.wikipedia.org/wiki/Human_behavior\n",
      "https://en.wikipedia.org/wiki/Human\n",
      "https://en.wikipedia.org/wiki/Extant_taxon\n",
      "https://en.wikipedia.org/wiki/Biology\n",
      "https://en.wikipedia.org/wiki/Natural_science\n",
      "https://en.wikipedia.org/wiki/Branches_of_science\n",
      "https://en.wikipedia.org/wiki/Science\n",
      "https://en.wikipedia.org/wiki/Latin\n",
      "https://en.wikipedia.org/wiki/Classical_language\n",
      "https://en.wikipedia.org/wiki/Language\n",
      "https://en.wikipedia.org/wiki/Grammar\n",
      "https://en.wikipedia.org/wiki/Linguistics\n",
      "We've arrived at an article we've already seen, aborting search!\n"
     ]
    }
   ],
   "source": [
    "# 测试程序\n",
    "start_url = 'https://en.wikipedia.org/wiki/Film'\n",
    "target_url = \"https://en.wikipedia.org/wiki/Philosophy\"\n",
    "\n",
    "web_crawl(start_url, target_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
